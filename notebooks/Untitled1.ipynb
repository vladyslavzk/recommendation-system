{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3911d5b6-0440-40a3-ac3e-5f21962b5eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "from src.data import BasicCleaner\n",
    "from src.data import ContentBasedFeatures\n",
    "\n",
    "from src.models.content_based import ContentBasedRecommender\n",
    "\n",
    "import scipy\n",
    "\n",
    "import sklearn.preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f7bac36-e317-4474-bedd-7bed24c33322",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d75924f8-7501-45f9-a1b8-1b2652afae98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I:\\\\learning_projects\\\\projects\\\\ecommerce\\\\hadoop'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.abspath('../hadoop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc527763-179d-49d2-84aa-2985db27a42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "os.environ['HADOOP_HOME'] = '../hadoop'\n",
    "os.environ['PATH'] = f\"{os.environ['HADOOP_HOME']}/bin;\" + os.environ['PATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2bcf280-5150-4621-90dc-407810f6f3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"EcommerceAnalysis\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.local.dir\", \"I:/spark_temp\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"10\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03c2d24b-fbb7-40e0-9f81-893ee3f718ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('../data/raw/2019-Oct.csv', header=True, inferSchema=True).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84063c6f-34ee-4f21-a8c2-12cea38beee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.data.basic_cleaning:Cleaning DataFrame...\n",
      "INFO:src.data.basic_cleaning:Removing duplicates from DataFrame...\n",
      "INFO:src.data.basic_cleaning:Price filtering...\n",
      "INFO:src.data.basic_cleaning:Handling missing category_code...\n",
      "INFO:src.data.basic_cleaning:Handling missing brands...\n"
     ]
    }
   ],
   "source": [
    "cleaner = BasicCleaner()\n",
    "df = cleaner.clean_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d4890e-2752-40dd-8f25-885a9b68260e",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_df = df.limit(10000)\n",
    "small_df.write.parquet('temp_data.parquet', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72bd0813-2a70-4417-a050-4c4d95d56396",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_df = spark.read.parquet('temp_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8c2a404-c024-404f-91b8-3b94003b3989",
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_feature_transformer = ContentBasedFeatures(spark)\n",
    "model = ContentBasedRecommender(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ffc1667-6de9-4736-bc9a-c6c7710fd8d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.data.feature_engineering:Creating weighted category paths...\n",
      "INFO:src.data.feature_engineering:Training Word2Vec model...\n",
      "INFO:gensim.models.word2vec:collecting all words and their counts\n",
      "INFO:gensim.models.word2vec:PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO:gensim.models.word2vec:collected 401 word types from a corpus of 401 raw words and 401 sentences\n",
      "INFO:gensim.models.word2vec:Creating a fresh vocabulary\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'effective_min_count=1 retains 401 unique words (100.00% of original 401, drops 0)', 'datetime': '2024-12-25T12:44:41.009865', 'gensim': '4.3.3', 'python': '3.11.8 (tags/v3.11.8:db85d51, Feb  6 2024, 22:03:32) [MSC v.1937 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 401 word corpus (100.00% of original 401, drops 0)', 'datetime': '2024-12-25T12:44:41.010865', 'gensim': '4.3.3', 'python': '3.11.8 (tags/v3.11.8:db85d51, Feb  6 2024, 22:03:32) [MSC v.1937 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "INFO:gensim.models.word2vec:deleting the raw counts dictionary of 401 items\n",
      "INFO:gensim.models.word2vec:sample=0.001 downsamples 0 most-common words\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 401 word corpus (100.0%% of prior 401)', 'datetime': '2024-12-25T12:44:41.014869', 'gensim': '4.3.3', 'python': '3.11.8 (tags/v3.11.8:db85d51, Feb  6 2024, 22:03:32) [MSC v.1937 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'prepare_vocab'}\n",
      "INFO:gensim.models.word2vec:estimated required memory for 401 words and 32 dimensions: 303156 bytes\n",
      "INFO:gensim.models.word2vec:resetting layer weights\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2024-12-25T12:44:41.020875', 'gensim': '4.3.3', 'python': '3.11.8 (tags/v3.11.8:db85d51, Feb  6 2024, 22:03:32) [MSC v.1937 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'build_vocab'}\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'training model with 4 workers on 401 vocabulary and 32 features, using sg=1 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2024-12-25T12:44:41.020875', 'gensim': '4.3.3', 'python': '3.11.8 (tags/v3.11.8:db85d51, Feb  6 2024, 22:03:32) [MSC v.1937 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'train'}\n",
      "INFO:gensim.models.word2vec:EPOCH 0: training on 401 raw words (401 effective words) took 0.0s, 771748 effective words/s\n",
      "INFO:gensim.models.word2vec:EPOCH 1: training on 401 raw words (401 effective words) took 0.0s, 557254 effective words/s\n",
      "INFO:gensim.models.word2vec:EPOCH 2: training on 401 raw words (401 effective words) took 0.0s, 517419 effective words/s\n",
      "INFO:gensim.models.word2vec:EPOCH 3: training on 401 raw words (401 effective words) took 0.0s, 409225 effective words/s\n",
      "INFO:gensim.models.word2vec:EPOCH 4: training on 401 raw words (401 effective words) took 0.0s, 557254 effective words/s\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'msg': 'training on 2005 raw words (2005 effective words) took 0.0s, 94807 effective words/s', 'datetime': '2024-12-25T12:44:41.042894', 'gensim': '4.3.3', 'python': '3.11.8 (tags/v3.11.8:db85d51, Feb  6 2024, 22:03:32) [MSC v.1937 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'train'}\n",
      "INFO:gensim.utils:Word2Vec lifecycle event {'params': 'Word2Vec<vocab=401, vector_size=32, alpha=0.025>', 'datetime': '2024-12-25T12:44:41.043896', 'gensim': '4.3.3', 'python': '3.11.8 (tags/v3.11.8:db85d51, Feb  6 2024, 22:03:32) [MSC v.1937 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19045-SP0', 'event': 'created'}\n",
      "INFO:src.data.feature_engineering:Adding embedding features to DataFrame...\n",
      "INFO:src.data.feature_engineering:Creating price-based features...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------------+\n",
      "|category_code|category_embedding|\n",
      "+-------------+------------------+\n",
      "+-------------+------------------+\n",
      "\n",
      "Feature columns: ['category_emb_0', 'category_emb_1', 'category_emb_2', 'category_emb_3', 'category_emb_4', 'category_emb_5', 'category_emb_6', 'category_emb_7', 'category_emb_8', 'category_emb_9', 'category_emb_10', 'category_emb_11', 'category_emb_12', 'category_emb_13', 'category_emb_14', 'category_emb_15', 'category_emb_16', 'category_emb_17', 'category_emb_18', 'category_emb_19', 'category_emb_20', 'category_emb_21', 'category_emb_22', 'category_emb_23', 'category_emb_24', 'category_emb_25', 'category_emb_26', 'category_emb_27', 'category_emb_28', 'category_emb_29', 'category_emb_30', 'category_emb_31', 'price_normalized']\n",
      "Columns in df_aggregated: ['product_id', 'category_embedding', 'price_normalized', 'category_emb_0', 'category_emb_1', 'category_emb_2', 'category_emb_3', 'category_emb_4', 'category_emb_5', 'category_emb_6', 'category_emb_7', 'category_emb_8', 'category_emb_9', 'category_emb_10', 'category_emb_11', 'category_emb_12', 'category_emb_13', 'category_emb_14', 'category_emb_15', 'category_emb_16', 'category_emb_17', 'category_emb_18', 'category_emb_19', 'category_emb_20', 'category_emb_21', 'category_emb_22', 'category_emb_23', 'category_emb_24', 'category_emb_25', 'category_emb_26', 'category_emb_27', 'category_emb_28', 'category_emb_29', 'category_emb_30', 'category_emb_31']\n"
     ]
    }
   ],
   "source": [
    "df_train = cb_feature_transformer.create_features(small_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc7dd3d8-063c-4e93-a88b-a6ae08bc76ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.filter(df_train.product_id == 5100816).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a54e848e-0e36-4541-bfd7-b47ce8de1d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.models.content_based:Preparing feature vectors...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in DataFrame: 5854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.models.content_based:Calculating similarity matrix...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of items collected: 5854\n",
      "First few items: [(1002099, array([ 0.01956469, -0.01584572,  0.00852447,  0.00795345,  0.00749321,\n",
      "        0.0260408 , -0.02160443, -0.00718572,  0.00315297,  0.01993049,\n",
      "       -0.00202729,  0.02477537, -0.01453724, -0.02822312, -0.02204406,\n",
      "       -0.0129303 ,  0.02994781, -0.0160227 , -0.01515113,  0.02554213,\n",
      "       -0.02330058,  0.0062805 ,  0.01786264, -0.0235323 , -0.0008218 ,\n",
      "       -0.00740059,  0.01739257, -0.01425141, -0.02063996,  0.03091168,\n",
      "       -0.01444734,  0.02376332,  0.14348958])), (1002544, array([ 0.01956469, -0.01584572,  0.00852447,  0.00795345,  0.00749321,\n",
      "        0.0260408 , -0.02160443, -0.00718572,  0.00315297,  0.01993049,\n",
      "       -0.00202729,  0.02477537, -0.01453724, -0.02822312, -0.02204406,\n",
      "       -0.0129303 ,  0.02994781, -0.0160227 , -0.01515113,  0.02554213,\n",
      "       -0.02330058,  0.0062805 ,  0.01786264, -0.0235323 , -0.0008218 ,\n",
      "       -0.00740059,  0.01739257, -0.01425141, -0.02063996,  0.03091168,\n",
      "       -0.01444734,  0.02376332,  0.17852716])), (1002665, array([ 0.01956469, -0.01584572,  0.00852447,  0.00795345,  0.00749321,\n",
      "        0.0260408 , -0.02160443, -0.00718572,  0.00315297,  0.01993049,\n",
      "       -0.00202729,  0.02477537, -0.01453724, -0.02822312, -0.02204406,\n",
      "       -0.0129303 ,  0.02994781, -0.0160227 , -0.01515113,  0.02554213,\n",
      "       -0.02330058,  0.0062805 ,  0.01786264, -0.0235323 , -0.0008218 ,\n",
      "       -0.00740059,  0.01739257, -0.01425141, -0.02063996,  0.03091168,\n",
      "       -0.01444734,  0.02376332,  0.08955224]))]\n",
      "Number of unique product_ids: 5854\n",
      "Number of items in mappings: 5854\n",
      "First few mapping items: [(1002099, 0), (1002544, 1), (1002665, 2)]\n",
      "Number of items in mappings: 5854\n",
      "First few mapping idx: [(0, 1002099), (1, 1002544), (2, 1002665)]\n"
     ]
    }
   ],
   "source": [
    "model.fit(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9381a3c-e495-4e47-8e84-ca073705a058",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5100767, 0.9999902823970895),\n",
       " (5100768, 0.9999902823970895),\n",
       " (5100871, 0.9999838860754093),\n",
       " (5100781, 0.9999384049840955),\n",
       " (5100607, 0.9999077712579397)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_similar_products(5100816)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5e5aed2e-e87b-467c-b138-c733d60a1eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Brand-Category Analysis Results ===\n",
      "\n",
      "Basic Statistics:\n",
      "Total unique brands: 867\n",
      "Total unique categories: 368\n",
      "Average categories per brand: 2.03\n",
      "Median categories per brand: 1.00\n",
      "\n",
      "1. Overall Category-Brand Correlation:\n",
      "Normalized Mutual Information Score: 0.6250\n",
      "\n",
      "2. Top 10 Most Category-Focused Brands:\n",
      "xiaomi: 0.8600\n",
      "samsung: 0.8402\n",
      "apple: 0.8115\n",
      "huawei: 0.7744\n",
      "asus: 0.6894\n",
      "hp: 0.6798\n",
      "acer: 0.6792\n",
      "unknown: 0.6666\n",
      "respect: 0.6447\n",
      "lenovo: 0.6368\n",
      "\n",
      "3. Category Exclusivity Analysis:\n",
      "\n",
      "Top 10 Category-Exclusive Brands (with significant presence):\n",
      "huawei: electronics.smartphone (91.70% of activity)\n",
      "asus: computers.notebook (81.82% of activity)\n",
      "apple: electronics.smartphone (80.21% of activity)\n",
      "acer: computers.notebook (73.12% of activity)\n",
      "samsung: electronics.smartphone (72.75% of activity)\n",
      "respect: apparel.shoes (72.37% of activity)\n",
      "xiaomi: electronics.smartphone (68.99% of activity)\n",
      "hp: computers.notebook (54.55% of activity)\n",
      "beko: appliances.kitchen.refrigerators (46.15% of activity)\n",
      "midea: appliances.kitchen.washer (45.83% of activity)\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "def analyze_brand_category_relationships(df):\n",
    "    \"\"\"Analyze relationships between brands and categories\"\"\"\n",
    "    \n",
    "    # Standardize brand names to lowercase\n",
    "    df = df.withColumn('brand_std', F.lower(F.col('brand')))\n",
    "    \n",
    "    # 1. Basic co-occurrence analysis\n",
    "    brand_category_counts = df.groupBy('brand_std', 'category_code') \\\n",
    "        .count() \\\n",
    "        .orderBy('count', ascending=False)\n",
    "    \n",
    "    # Convert to pandas for easier analysis\n",
    "    brand_cat_matrix = brand_category_counts.toPandas() \\\n",
    "        .pivot(index='brand_std', columns='category_code', values='count') \\\n",
    "        .fillna(0)\n",
    "    \n",
    "    # 2. Calculate brand category concentration\n",
    "    brand_concentration = {}\n",
    "    for brand in brand_cat_matrix.index:\n",
    "        distribution = brand_cat_matrix.loc[brand]\n",
    "        # Calculate Gini coefficient\n",
    "        sorted_dist = np.sort(distribution[distribution > 0])\n",
    "        n = len(sorted_dist)\n",
    "        if n > 0:\n",
    "            index = np.arange(1, n + 1)\n",
    "            brand_concentration[brand] = ((np.sum((2 * index - n - 1) * sorted_dist)) / \n",
    "                                       (n * np.sum(sorted_dist)))\n",
    "    \n",
    "    # 3. Category exclusivity for brands\n",
    "    brand_exclusivity = {}\n",
    "    total_categories = brand_cat_matrix.astype(bool).sum(axis=1)\n",
    "    for brand in brand_cat_matrix.index:\n",
    "        top_category = brand_cat_matrix.loc[brand].idxmax()\n",
    "        category_share = (brand_cat_matrix.loc[brand][top_category] / \n",
    "                        brand_cat_matrix.loc[brand].sum())\n",
    "        brand_exclusivity[brand] = {\n",
    "            'top_category': top_category,\n",
    "            'category_share': category_share,\n",
    "            'total_categories': total_categories[brand]\n",
    "        }\n",
    "    \n",
    "    # 4. Calculate normalized mutual information\n",
    "    # Convert Row objects to values properly\n",
    "    brand_indices = {row['brand_std']: idx for idx, row in \n",
    "                    enumerate(df.select('brand_std').distinct().collect())}\n",
    "    category_indices = {row['category_code']: idx for idx, row in \n",
    "                       enumerate(df.select('category_code').distinct().collect())}\n",
    "    \n",
    "    # Create arrays using proper Row object access\n",
    "    brand_arr = [brand_indices[row['brand_std']] for row in df.select('brand_std').collect()]\n",
    "    category_arr = [category_indices[row['category_code']] for row in df.select('category_code').collect()]\n",
    "    \n",
    "    nmi_score = normalized_mutual_info_score(brand_arr, category_arr)\n",
    "    \n",
    "    # Add some basic statistics\n",
    "    stats = {\n",
    "        'total_brands': len(brand_indices),\n",
    "        'total_categories': len(category_indices),\n",
    "        'avg_categories_per_brand': total_categories.mean(),\n",
    "        'median_categories_per_brand': total_categories.median()\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        'brand_concentration': brand_concentration,\n",
    "        'brand_exclusivity': brand_exclusivity,\n",
    "        'nmi_score': nmi_score,\n",
    "        'contingency_matrix': brand_cat_matrix,\n",
    "        'stats': stats\n",
    "    }\n",
    "\n",
    "def get_top_category_brands(df, top_n=10):\n",
    "    \"\"\"Get top brands for each category\"\"\"\n",
    "    return df.groupBy('category_code', 'brand_std') \\\n",
    "        .count() \\\n",
    "        .orderBy(['category_code', 'count'], ascending=[True, False]) \\\n",
    "        .groupBy('category_code') \\\n",
    "        .agg(F.collect_list(F.struct('brand_std', 'count')).alias('brands')) \\\n",
    "        .rdd.map(lambda x: (x.category_code, x.brands[:top_n])) \\\n",
    "        .collectAsMap()\n",
    "\n",
    "def print_analysis_results(results):\n",
    "    \"\"\"Print readable analysis results\"\"\"\n",
    "    print(\"=== Brand-Category Analysis Results ===\")\n",
    "    \n",
    "    print(\"\\nBasic Statistics:\")\n",
    "    print(f\"Total unique brands: {results['stats']['total_brands']}\")\n",
    "    print(f\"Total unique categories: {results['stats']['total_categories']}\")\n",
    "    print(f\"Average categories per brand: {results['stats']['avg_categories_per_brand']:.2f}\")\n",
    "    print(f\"Median categories per brand: {results['stats']['median_categories_per_brand']:.2f}\")\n",
    "    \n",
    "    print(\"\\n1. Overall Category-Brand Correlation:\")\n",
    "    print(f\"Normalized Mutual Information Score: {results['nmi_score']:.4f}\")\n",
    "    \n",
    "    print(\"\\n2. Top 10 Most Category-Focused Brands:\")\n",
    "    sorted_brands = sorted(results['brand_concentration'].items(), \n",
    "                         key=lambda x: x[1], reverse=True)[:10]\n",
    "    for brand, concentration in sorted_brands:\n",
    "        print(f\"{brand}: {concentration:.4f}\")\n",
    "    \n",
    "    print(\"\\n3. Category Exclusivity Analysis:\")\n",
    "    exclusive_brands = sorted(\n",
    "        [(brand, info) for brand, info in results['brand_exclusivity'].items()\n",
    "         if info['category_share'] > 0.2 and info['total_categories'] > 5],\n",
    "        key=lambda x: x[1]['category_share'], \n",
    "        reverse=True\n",
    "    )[:10]\n",
    "    \n",
    "    print(\"\\nTop 10 Category-Exclusive Brands (with significant presence):\")\n",
    "    for brand, info in exclusive_brands:\n",
    "        print(f\"{brand}: {info['top_category']} ({info['category_share']:.2%} of activity)\")\n",
    "\n",
    "# Example usage\n",
    "results = analyze_brand_category_relationships(small_df)\n",
    "print_analysis_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c15423-47a6-448e-b7a5-65e204bd1315",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
